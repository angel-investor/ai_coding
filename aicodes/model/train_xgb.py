"""
XGBoost åˆ†ç±»æ¨¡å‹è®­ç»ƒ
å¿ƒè¡€ç®¡ç–¾ç—…é¢„æµ‹
"""

import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    classification_report,
    confusion_matrix,
    roc_auc_score
)
import joblib
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.logger import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger('train_xgb', log_dir='./logs')


class XGBoostTrainer:
    """XGBoost æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, data_path: str, target_col: str = 'cardio'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            target_col: ç›®æ ‡åˆ—å
        """
        self.data_path = data_path
        self.target_col = target_col
        self.model = None
        self.scaler = None
        self.feature_names = None
        
        logger.info(f"åˆå§‹åŒ– XGBoost è®­ç»ƒå™¨")
        logger.info(f"æ•°æ®è·¯å¾„: {data_path}")
        logger.info(f"ç›®æ ‡åˆ—: {target_col}")
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        logger.info("åŠ è½½æ•°æ®...")
        
        try:
            if self.data_path.endswith('.xlsx'):
                df = pd.read_excel(self.data_path)
            elif self.data_path.endswith('.csv'):
                df = pd.read_csv(self.data_path)
            else:
                raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            
            logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def preprocess_data(self, df):
        """
        æ•°æ®é¢„å¤„ç†
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
        """
        logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        if self.target_col not in df.columns:
            raise ValueError(f"ç›®æ ‡åˆ— '{self.target_col}' ä¸å­˜åœ¨")
        
        # æ’é™¤ id åˆ—å’Œç›®æ ‡åˆ—
        exclude_cols = [self.target_col]
        if 'id' in df.columns:
            exclude_cols.append('id')
            logger.info("æ’é™¤ 'id' åˆ—")
        
        X = df.drop(columns=exclude_cols)
        y = df[self.target_col]
        
        # ä¿å­˜ç‰¹å¾å
        self.feature_names = X.columns.tolist()
        logger.info(f"ç‰¹å¾åˆ—: {self.feature_names}")
        
        # å¤„ç†ç¼ºå¤±å€¼
        if X.isnull().sum().sum() > 0:
            logger.warning("å‘ç°ç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
            X = X.fillna(X.mean())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ç±»å˜é‡éœ€è¦ one-hot ç¼–ç 
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if categorical_cols:
            logger.info(f"å¯¹åˆ†ç±»å˜é‡è¿›è¡Œ one-hot ç¼–ç : {categorical_cols}")
            X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
            self.feature_names = X.columns.tolist()
        
        logger.info(f"é¢„å¤„ç†åç‰¹å¾æ•°: {X.shape[1]}")
        logger.info(f"æ ·æœ¬æ•°: {X.shape[0]}")
        logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        return X, y
    
    def split_data(self, X, y, test_size=0.2, random_state=42):
        """
        åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        logger.info(f"åˆ’åˆ†æ•°æ®é›†ï¼Œæµ‹è¯•é›†æ¯”ä¾‹: {test_size}")
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=test_size, 
            random_state=random_state,
            stratify=y  # ä¿æŒç±»åˆ«æ¯”ä¾‹
        )
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
        
        return X_train, X_test, y_train, y_test
    
    def standardize_features(self, X_train, X_test):
        """
        æ ‡å‡†åŒ–ç‰¹å¾
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            
        Returns:
            X_train_scaled, X_test_scaled
        """
        logger.info("æ ‡å‡†åŒ–ç‰¹å¾...")
        
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        logger.info("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        
        return X_train_scaled, X_test_scaled
    
    def train_model(self, X_train, y_train, **params):
        """
        è®­ç»ƒ XGBoost æ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†ç›®æ ‡
            **params: XGBoost å‚æ•°
        """
        logger.info("å¼€å§‹è®­ç»ƒ XGBoost æ¨¡å‹...")
        
        # é»˜è®¤å‚æ•°
        default_params = {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'eval_metric': 'logloss',
            'use_label_encoder': False
        }
        
        # æ›´æ–°å‚æ•°
        default_params.update(params)
        
        logger.info(f"æ¨¡å‹å‚æ•°: {default_params}")
        
        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        self.model = XGBClassifier(**default_params)
        self.model.fit(X_train, y_train)
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def evaluate_model(self, X_test, y_test):
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            X_test: æµ‹è¯•é›†ç‰¹å¾
            y_test: æµ‹è¯•é›†ç›®æ ‡
            
        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        logger.info("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba)
        }
        
        # æ‰“å°ç»“æœ
        print("\n" + "=" * 50)
        print("æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("=" * 50)
        print(f"Accuracy:  {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall:    {metrics['recall']:.4f}")
        print(f"F1 Score:  {metrics['f1_score']:.4f}")
        print(f"ROC AUC:   {metrics['roc_auc']:.4f}")
        print("=" * 50)
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['å¥åº·', 'æ‚£ç—…']))
        
        # æ··æ·†çŸ©é˜µ
        print("\næ··æ·†çŸ©é˜µ:")
        cm = confusion_matrix(y_test, y_pred)
        print(cm)
        print(f"çœŸé˜´æ€§: {cm[0,0]}, å‡é˜³æ€§: {cm[0,1]}")
        print(f"å‡é˜´æ€§: {cm[1,0]}, çœŸé˜³æ€§: {cm[1,1]}")
        
        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"æ¨¡å‹è¯„ä¼°å®Œæˆ")
        for metric, value in metrics.items():
            logger.info(f"{metric}: {value:.4f}")
        
        return metrics
    
    def save_model(self, model_dir='./model'):
        """
        ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        logger.info(f"ä¿å­˜æ¨¡å‹åˆ°: {model_dir}")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(model_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(model_dir, 'xgb_model.pkl')
        joblib.dump(self.model, model_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = os.path.join(model_dir, 'scaler.pkl')
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
        
        # ä¿å­˜ç‰¹å¾å
        features_path = os.path.join(model_dir, 'feature_names.pkl')
        joblib.dump(self.feature_names, features_path)
        logger.info(f"ç‰¹å¾åå·²ä¿å­˜: {features_path}")
        
        print(f"\nâœ… æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {os.path.abspath(model_dir)}")
    
    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
        print(importance_df.head(10).to_string(index=False))
        
        return importance_df
    
    def run_full_pipeline(self, test_size=0.2, **model_params):
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Args:
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            **model_params: æ¨¡å‹å‚æ•°
        """
        logger.info("=" * 50)
        logger.info("å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹")
        logger.info("=" * 50)
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_data()
        
        # 2. é¢„å¤„ç†
        X, y = self.preprocess_data(df)
        
        # 3. åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = self.split_data(X, y, test_size)
        
        # 4. æ ‡å‡†åŒ–
        X_train_scaled, X_test_scaled = self.standardize_features(X_train, X_test)
        
        # 5. è®­ç»ƒæ¨¡å‹
        self.train_model(X_train_scaled, y_train, **model_params)
        
        # 6. è¯„ä¼°æ¨¡å‹
        metrics = self.evaluate_model(X_test_scaled, y_test)
        
        # 7. ç‰¹å¾é‡è¦æ€§
        self.get_feature_importance()
        
        # 8. ä¿å­˜æ¨¡å‹
        self.save_model()
        
        logger.info("=" * 50)
        logger.info("è®­ç»ƒæµç¨‹å®Œæˆ")
        logger.info("=" * 50)
        
        return metrics


def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®è·¯å¾„
    data_path = "D:/project/workspace/ai_coding/data/å¿ƒè¡€ç®¡ç–¾ç—….xlsx"
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = XGBoostTrainer(data_path, target_col='cardio')
    
    # è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
    metrics = trainer.run_full_pipeline(
        test_size=0.2,
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8
    )
    
    print("\n" + "=" * 50)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 50)
    print("\nä¸‹ä¸€æ­¥:")
    print("1. æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶: model/xgb_model.pkl")
    print("2. å¯åŠ¨ Flask æœåŠ¡: python run_server.py")
    print("3. è®¿é—®é¢„æµ‹é¡µé¢: http://localhost:5000/web/predict.html")
    print("=" * 50 + "\n")


if __name__ == '__main__':
    main()

